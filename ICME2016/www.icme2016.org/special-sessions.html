<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" itemscope="" itemtype="http://schema.org/WebPage">

<!-- Mirrored from www.icme2014.org/special-sessions by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 03 Jul 2014 11:52:07 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv="X-UA-Compatible" content="chrome=1" />
<script type="text/javascript">/* Copyright 2008 Google. */ (function() { var b=window,e="chrome",g="tick",h="wtsrt_",l="tbsd_",m="tbnd_",n="start",p="_wtsrt",q="_tbnd",r="CSI/index.html";(function(){function k(a){this.t={};this.tick=function(a,d,c){this.t[a]=[void 0!=c?c:(new Date).getTime(),d];if(void 0==c)try{b.console.timeStamp(r+a)}catch(f){}};this[g](n,null,a)}var a;b.performance&&(a=b.performance.timing);var s=a?new k(a.responseStart):new k;b.jstiming={Timer:k,load:s};if(a){var d=a.navigationStart,f=a.responseStart;0<d&&f>=d&&(b.jstiming.srt=f-d)}if(a){var c=b.jstiming.load;0<d&&f>=d&&(c[g](p,void 0,d),c[g](h,p,f),c[g](l,h))}try{a=null,b[e]&&b[e].csi&&(a=Math.floor(b[e].csi().pageT),
c&&0<d&&(c[g](q,void 0,b[e].csi().startE),c[g](m,q,d))),null==a&&b.gtbExternal&&(a=b.gtbExternal.pageT()),null==a&&b.external&&(a=b.external.pageT,c&&0<d&&(c[g](q,void 0,b.external.startE),c[g](m,q,d))),a&&(b.jstiming.pt=a)}catch(t){}})(); })()
</script>
<link rel="shortcut icon" type="image/x-icon" href="http://www.google.com/images/icons/product/sites-16.ico" />
<link rel="apple-touch-icon" href="../www.gstatic.com/sites/p/158cce/system/app/images/apple-touch-icon.png" type="image/png" />
<script type="text/javascript">/* Copyright 2008 Google. */ (function() { var d=window,e="length",h="",k="__duration__",l="function";function m(c){return document.getElementById(c)}d.byId=m;function n(c){return c.replace(/^\s+|\s+$/g,h)}d.trim=n;var p=[],q=0;d.JOT_addListener=function(c,a,b){var f=new String(q++);c={eventName:c,handler:a,compId:b,key:f};p.push(c);return f};d.JOT_removeListenerByKey=function(c){for(var a=0;a<p[e];a++)if(p[a].key==c){p.splice(a,1);break}};d.JOT_removeAllListenersForName=function(c){for(var a=0;a<p[e];a++)p[a].eventName==c&&p.splice(a,1)};
d.JOT_postEvent=function(c,a,b){var f={eventName:c,eventSrc:a||{},payload:b||{}};if(d.JOT_fullyLoaded)for(a=p[e],b=0;b<a&&b<p[e];b++){var g=p[b];g&&g.eventName==c&&(f.listenerCompId=g.compId||h,(g=typeof g.handler==l?g.handler:d[g.handler])&&g(f))}else d.JOT_delayedEvents.push({eventName:c,eventSrc:a,payload:b})};d.JOT_delayedEvents=[];d.JOT_fullyLoaded=!1;
d.JOT_formatRelativeToNow=function(c,a){var b=((new Date).getTime()-c)/6E4;if(1440<=b||0>b)return null;var f=0;60<=b&&(b/=60,f=2);2<=b&&f++;return a?d.JOT_siteRelTimeStrs[f].replace(k,Math.floor(b)):d.JOT_userRelTimeStrs[f].replace(k,Math.floor(b))}; })()
</script>
<script>


var webspace = {"scottyUrl":"/_/upload","isConsumer":true,"canPublishScriptToAnyone":true,"serverFlags":{"cajaBaseUrl":"//www.gstatic.com/caja","cajaDebugMode":false},"sharingUrlPrefix":"/_/sharing","csiReportUri":"http://csi.gstatic.com/csi","sharingPolicy":"OPENED","analyticsAccountId":"","baseUri":"","enableUniversalAnalytics":false,"name":"ieeeicme2014","features":{"copySiteProgressUpdates":true,"horizontalNavLayout":true,"asyncPermanentDelete":false,"ritzSupport":true,"folderEmbed":true,"moreBackgroundTweaks":true,"flipFolderUrls":true,"animateNavigation":true,"driveInFileCabinet":true,"skiThemeIsDefault":true,"customHeaderThemes":true,"pdfEmbedSupport":false,"siteChromeSidebarWidgetsEditDialog":true,"animateToc":true,"canonicalLinkTagInHead":true,"analyticsTrackingForCorp":false,"driveImageEmbed":true,"siteChromeSystemFooterDialog":true,"plusOneButtonOptions":true,"plusOneButton":true,"adSenseDeprecate":true,"newKeyboardShortcuts":true,"mapsEngineEmbeds":true,"photoAlbumsInOnePick":true,"docosHideNotificationSettings":true,"helpBox":true,"openEditorMenuWithAltShift":true,"siteNotice":true,"skiTheme":true,"siteChromeDialogsToolbar":true,"plusBadge":false,"ritzChartSupport":true,"sitesLoveFixes2014Q2":true,"siteChromeHorizontalNavigationDialog":true,"youTubeEmbedSize":true,"linkPickerKeyboardShortcuts":true,"adSenseDeprecateMsg":true,"htmlEmbed":true,"publishDraftWithSaveShortcut":true,"newEditorMenuKeyboardHandling":true,"plusPost":true,"photoAlbumsGPlusUrlSupport":true,"siteChromeHeaderDialog":true,"fileCabinetOptions":true,"sitesLoveFixes":true},"domain":"defaultdomain","adsensePublisherId":null,"gvizVersion":1,"siteTitle":"ICME 2014","pageSharingId":"jotspot_page","plusPageId":"","onepickBaseUrl":"https://docs.google.com","siteNoticeRevision":null,"termsUrl":"http://sites.google.com/site/sites/system/app/pages/meta/terms","enableAnalytics":false,"siteNoticeMessage":null,"isPublic":true,"plusPageUrl":"","homePath":"/","sharingId":"jotspot","isAdsenseEnabled":true,"adsensePromoClickedOrSiteIneligible":true,"isStartPageEnabled":false,"domainAnalyticsAccountId":""};



webspace.gadgets = {"baseUri":"/system/app/pages/gadgets"};


webspace.user = {"uid":"","renderMobile":false,"namespaceUser":false,"sessionIndex":"","primaryEmail":"guest","displayNameOrEmail":"guest","namespace":"","hasAdminAccess":false,"guest_":true,"keyboardShortcuts":true,"hasWriteAccess":false,"domain":"","dasherUser":false,"userName":"guest"};

webspace.page = {"canDeleteWebspace":null,"locale":"en","state":"","wuid":"wuid:gx:7a1e6d73564ec976","pageInheritsPermissions":null,"timeZone":"America/Los_Angeles","properties":{},"type":"text","canChangePath":true,"parentWuid":null,"revision":41,"title":"Special Sessions","isRtlLocale":false,"bidiEnabled":false,"siteLocale":"en","name":"special-sessions","path":"/special-sessions","isSiteRtlLocale":false,"parentPath":null};
webspace.page.breadcrumbs = [{"title":"Special Sessions","dir":"ltr","path":"/special-sessions","deleted":false}];


webspace.editorResources = {
  text: [
    'http://www.gstatic.com/sites/p/158cce/system/js/codemirror.js',
    '../www.gstatic.com/sites/p/158cce/system/app/css/codemirror_css.css',
    '../www.gstatic.com/sites/p/158cce/system/js/trog_edit__en.js',
    '../www.gstatic.com/sites/p/158cce/system/app/css/trogedit.css',
    '_/rsrc/1403685870000/system/app/css/editor.css',
    '../www.gstatic.com/sites/p/158cce/system/app/css/codeeditor.css',
    '_/rsrc/1403685870000/system/app/css/camelot/editor-jfk.css'
  ],
  sitelayout: [
    'http://www.gstatic.com/sites/p/158cce/system/app/css/sitelayouteditor.css'
  ]
};

var JOT_clearDotPath = '../www.gstatic.com/sites/p/158cce/system/app/images/cleardot.gif';


var JOT_userRelTimeStrs = ["a minute ago","__duration__ minutes ago","an hour ago","__duration__ hours ago"];


webspace.siteTemplateId = false;


webspace.page.currentTemplate = {"title":"Web Page","path":"/system/app/pagetemplates/text"};



var JOT_siteRelTimeStrs = ["a minute ago","__duration__ minutes ago","an hour ago","__duration__ hours ago"];

</script>
<script type="text/javascript">
                window.jstiming.load.tick('scl');
              </script>
<link rel="canonical" href="special-sessions.html" />
<meta name="title" content="Special Sessions - ICME 2014" />
<meta itemprop="name" content="Special Sessions - ICME 2014" />
<meta property="og:title" content="Special Sessions - ICME 2014" />
<meta name="description" content="The 2014 IEEE International Conference on Multimedia &amp; Expo" />
<meta itemprop="description" content="The 2014 IEEE International Conference on Multimedia &amp; Expo" />
<meta id="meta-tag-description" property="og:description" content="The 2014 IEEE International Conference on Multimedia &amp; Expo" />
<style type="text/css">
</style>
<link rel="stylesheet" type="text/css" href="../www.gstatic.com/sites/p/158cce/system/app/themes/solitudenavy/standard-css-solitudenavy-ltr-ltr.css" />
<link rel="stylesheet" type="text/css" href="_/rsrc/1403685870000/system/app/css/overlay6de5.css?cb=solitudenavy1000px200goog-ws-leftcontent30middlecenter" />
<link rel="stylesheet" type="text/css" href="_/rsrc/1403685870000/system/app/css/camelot/allthemes-view.css" />
<!--[if IE]>
          <link rel="stylesheet" type="text/css" href="/system/app/css/camelot/allthemes%2die.css" />
        <![endif]-->
<title>Special Sessions - ICME 2014</title>
<meta itemprop="image" content="/_/rsrc/1340688868168/config/customLogo.gif?revision=2" />
<meta property="og:image" content="/_/rsrc/1340688868168/config/customLogo.gif?revision=2" />
<script type="text/javascript">
                window.jstiming.load.tick('cl');
              </script>
</head>
<body xmlns="http://www.google.com/ns/jotspot" id="body" class=" en            ">
<script src="../www.gstatic.com/caja/5678/caja.js"> </script>
<script src="../www.gstatic.com/sites/p/158cce/system/js/jot_caja.js"> </script>
<div id="sites-page-toolbar" class="sites-header-divider">
<div xmlns="http://www.w3.org/1999/xhtml" id="sites-status" class="sites-status" style="display:none;"><div id="sites-notice" class="sites-notice" role="status" aria-live="assertive"> </div></div>
</div>
<div id="sites-chrome-everything-scrollbar">
<div id="sites-chrome-everything" class="">
<div id="sites-chrome-page-wrapper" style="direction: ltr">
<div id="sites-chrome-page-wrapper-inside">
<div xmlns="http://www.w3.org/1999/xhtml" id="sites-chrome-header-wrapper" style="height:auto;">
<table id="sites-chrome-header" class="sites-layout-hbox-centered" cellspacing="0" style="height:auto;">
<tr class="sites-header-primary-row" id="sites-chrome-userheader">
<td id="sites-header-title" class="sites-header-title-centered sites-chrome-header-valign-middle"><div class="sites-header-cell-buffer-wrapper"><a href="index.html" id="sites-chrome-userheader-logo"><img id="logo-img-id" src="_/rsrc/1340688868168/config/customLogo0dd8.png?revision=2" alt="ICME 2014" class="sites-logo-centered sites-chrome-header-valign-middle" /></a><h2></h2></div></td>
</tr>
<tr class="sites-header-secondary-row" id="sites-chrome-horizontal-nav">
<td colspan="2" id="sites-chrome-header-horizontal-nav-container">
</td>
</tr>
</table> 
</div> 
<div id="sites-chrome-main-wrapper">
<div id="sites-chrome-main-wrapper-inside">
<table id="sites-chrome-main" class="sites-layout-hbox" cellspacing="0" cellpadding="{scmCellpadding}" border="0">
<tr>
<td id="sites-chrome-sidebar-left" class="sites-layout-sidebar-left initial" style="width:200px">
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_2bd" class="sites-embed"><h4 class="sites-embed-title">Conference</h4><div class="sites-embed-content sites-sidebar-nav"><ul jotId="navList"><li class="nav-first "><div dir="ltr" style="padding-left: 5px;"><a href="home.html" jotId="wuid:gx:14d9b47a54ff2956" class="sites-navigation-link">Home</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="organizing-committee.html" jotId="wuid:gx:11992b6b263ea54" class="sites-navigation-link">Organizing Committee</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="important-dates.html" jotId="wuid:gx:c7361c9fbf7916e" class="sites-navigation-link">Important Dates</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="call-for-papers.html" jotId="wuid:gx:2817cfbec4293ddf" class="sites-navigation-link">Call for Papers</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="call-for-workshop-proposals.html" jotId="wuid:gx:1946b2399e7d3440" class="sites-navigation-link">Call for Workshop Proposals</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="call-for-tutorial-proposals.html" jotId="wuid:gx:617fead8135ef6f8" class="sites-navigation-link">Call for Tutorial Proposals</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="call-for-special-session-proposals.html" jotId="wuid:gx:1bc8879e3ba7d0b9" class="sites-navigation-link">Call for Special Session Proposals</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="call-for-demonstrations.html" jotId="wuid:gx:3a681cc0daa1a71a" class="sites-navigation-link">Call for Demonstrations</a></div></li></ul></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_39694875036366284" class="sites-embed"><h4 class="sites-embed-title">Program</h4><div class="sites-embed-content sites-sidebar-nav"><ul jotId="navList"><li class="nav-first "><div dir="ltr" style="padding-left: 5px;"><a href="https://sites.google.com/site/ieeeicme2014/program-at-a-glance.pdf?attredirects=0" class="sites-navigation-link">Program At A Glance</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="keynotes.html" jotId="wuid:gx:44bee28870bcc702" class="sites-navigation-link">Keynotes</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="oral-sessions.html" jotId="wuid:gx:1a8c879066c1b5c8" class="sites-navigation-link">Oral Sessions</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="poster-sessions.html" jotId="wuid:gx:572a2938bb6de30e" class="sites-navigation-link">Poster Sessions</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="special-sessions-1.html" jotId="wuid:gx:3608027b0d6c4874" class="sites-navigation-link">Special Sessions</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="demo-sessions.html" jotId="wuid:gx:6ac5d278b9a78fd5" class="sites-navigation-link">Demo Sessions</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="grand-challenge-session.html" jotId="wuid:gx:149ddcfab87495bb" class="sites-navigation-link">Grand Challenge Session</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="workshops.html" jotId="wuid:gx:25fa9b6c2a65da0b" class="sites-navigation-link">Workshops</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="doctoral-consortium.html" jotId="wuid:gx:3a5e57cc5d465afb" class="sites-navigation-link">Doctoral Consortium</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="tutorials.html" jotId="wuid:gx:20c5080b178e53a5" class="sites-navigation-link">Tutorials</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="panel-session.html" jotId="wuid:gx:ae239505a6c6c51" class="sites-navigation-link">Industry Forum and Panels</a></div></li></ul></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_14756362972374581" class="sites-embed"><h4 class="sites-embed-title">Industry Program</h4><div class="sites-embed-content sites-sidebar-nav"><ul jotId="navList" class="has-expander"><li class="topLevel nav-first "><div dir="ltr" style="padding-left: 19px;"><a href="industry-advisory-committee-iac.html" jotId="wuid:gx:766a3df64dec74e3" class="sites-navigation-link topLevel">Industry Advisory Committee (IAC)</a></div></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="call-for-industry-program.html" jotId="wuid:gx:1f8e47e709218fd0" class="sites-navigation-link topLevel">Call for Industry Programs</a></div></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="call-for-industry-sponsors.html" jotId="wuid:gx:5bc3c3d5039e60aa" class="sites-navigation-link topLevel">Call for Industry Sponsors</a></div></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="call-for-grand-challenges.html" jotId="wuid:gx:1b4afac42790ef95" class="sites-navigation-link topLevel">Call for Grand Challenges</a></div></li><li class="topLevel parent " wuid="gx:5e722d52ff2c866d"><div dir="ltr" style="padding-left: 0px;"><div class="expander"></div><a href="grand-challenges.html" jotId="wuid:gx:5e722d52ff2c866d" class="sites-navigation-link topLevel">Grand Challenges</a></div><ul class="has-expander"><li class=""><div dir="ltr" style="padding-left: 38px;"><a href="msr-bing-image-retrieval-challenge.html" jotId="wuid:gx:671681eaa044ad8e" class="sites-navigation-link">MSR-Bing Image Retrieval Challenge</a></div></li><li class=""><div dir="ltr" style="padding-left: 38px;"><a href="tencent-multimedia-ads-pctr-challenge.html" jotId="wuid:gx:86ed75161572455" class="sites-navigation-link">Tencent Multimedia Ads pCTR Challenge</a></div></li><li class=""><div dir="ltr" style="padding-left: 38px;"><a href="huawei-accurate-and-fast-mobile-video-annotation-challenge.html" jotId="wuid:gx:528f4ee192a0685e" class="sites-navigation-link">Huawei Accurate and Fast Mobile Video Annotation Challenge</a></div></li><li class=""><div dir="ltr" style="padding-left: 38px;"><a href="isvision-challenge.html" jotId="wuid:gx:2d2d1935b3d80663" class="sites-navigation-link">ISvision Challenge</a></div></li></ul></li></ul></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_5307298570405692" class="sites-embed"><h4 class="sites-embed-title">Author</h4><div class="sites-embed-content sites-sidebar-nav"><ul jotId="navList" class="has-expander"><li class="topLevel nav-first parent " wuid="gx:72491e2d5ea5951a"><div dir="ltr" style="padding-left: 0px;"><div class="expander"></div><a href="subject-areas.html" jotId="wuid:gx:72491e2d5ea5951a" class="sites-navigation-link topLevel">Subject Areas</a></div><ul class="has-expander"><li class=""><div class="current-bg" jotId="wuid:gx:7a1e6d73564ec976" dir="ltr" style="padding-left: 38px;">Special Sessions</div></li></ul></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="author-guidelines.html" jotId="wuid:gx:7e624f2dee0ff218" class="sites-navigation-link topLevel">Author Guidelines</a></div></li><li class="topLevel parent " wuid="gx:66e71ed7d6f6d68"><div dir="ltr" style="padding-left: 0px;"><div class="expander"></div><a href="paper-submission.html" jotId="wuid:gx:66e71ed7d6f6d68" class="sites-navigation-link topLevel">Paper Submission</a></div><ul class="has-expander"><li class=""><div dir="ltr" style="padding-left: 38px;"><a href="rebuttal.html" jotId="wuid:gx:735ab37428c0614" class="sites-navigation-link">Rebuttal</a></div></li></ul></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="camera-ready-submission-guidelines.html" jotId="wuid:gx:1d6365e210092197" class="sites-navigation-link topLevel">Camera-Ready Submission Guidelines</a></div></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="camera-ready-submission.html" jotId="wuid:gx:5d57c1554d6a28a2" class="sites-navigation-link topLevel">Camera-Ready Submission</a></div></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="copyright-submission.html" jotId="wuid:gx:20f881c21e0b2e72" class="sites-navigation-link topLevel">Copyright Submission</a></div></li><li class="topLevel "><div dir="ltr" style="padding-left: 19px;"><a href="presentation-guidelines.html" jotId="wuid:gx:61f1defd9c2f685c" class="sites-navigation-link topLevel">Presentation Guidelines</a></div></li></ul></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_7836366717237979" class="sites-embed"><h4 class="sites-embed-title">Attendee</h4><div class="sites-embed-content sites-sidebar-nav"><ul jotId="navList"><li class="nav-first "><div dir="ltr" style="padding-left: 5px;"><a href="registration.html" jotId="wuid:gx:106b750a37f7b9c8" class="sites-navigation-link">Registration</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="venue-hotels.html" jotId="wuid:gx:7357fa46a1bb849f" class="sites-navigation-link">Venue &amp; Hotels</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="travel.html" jotId="wuid:gx:6a0559b2a4701569" class="sites-navigation-link">Travel</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="food.html" jotId="wuid:gx:686f843f2acef1f3" class="sites-navigation-link">Food</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="city-of-chengdu.html" jotId="wuid:gx:26fff1ac17869958" class="sites-navigation-link">City of Chengdu</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="places-of-interest.html" jotId="wuid:gx:30d92fef776c86da" class="sites-navigation-link">Places of Interest</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="visa-information.html" jotId="wuid:gx:f942e9ae72c9d93" class="sites-navigation-link">Visa Information</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="chengdu-sightseeing-tours.html" jotId="wuid:gx:63f8bec695dc91e6" class="sites-navigation-link">Chengdu Sightseeing Tours</a></div></li></ul></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_5951473142486066" class="sites-embed"><h4 class="sites-embed-title">Other</h4><div class="sites-embed-content sites-sidebar-nav"><ul jotId="navList"><li class="nav-first "><div dir="ltr" style="padding-left: 5px;"><a href="sponsors-and-grand-challenge-hosts.html" jotId="wuid:gx:1185cd6059fc3fd" class="sites-navigation-link">Sponsors and Grand Challenge Hosts</a></div></li><li class=""><div dir="ltr" style="padding-left: 5px;"><a href="http://www.ieee-icme.org/index.php" class="sites-navigation-link">IEEE ICME</a></div></li></ul></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" class="sites-embed-align-left-wrapping-off" id="COMP_7142180013470352"><div class="sites-embed-border-on sites-embed sites-embed-full-width" style="width:100%;"><div class="sites-embed-content sites-embed-type-countdown"><div class="sites-embed-countdown sites-embed-countdown-fromdateutc-2014-7-14"><span jotid="content"><p style="text-align: center"><strong class="sites-embed-countdown-daysapart">11</strong>days until <br /><strong>ICME 2014</strong></p></span></div></div></div></div>
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_7310624034143984" class="sites-embed"><h4 class="sites-embed-title"></h4><div class="sites-embed-content sites-embed-content-sidebar-textbox"><div dir="ltr"><div style="display:block;text-align:left"><div style="display:block;text-align:left"><img border="0" src="_/rsrc/1340687183674/config/sponsors.png" /></div> </div><div style="display:block;text-align:left"></div><br /></div></div></div>
</td>
<td id="sites-canvas-wrapper">
<div id="sites-canvas">
<div id="goog-ws-editor-toolbar-container"> </div>
<div xmlns="http://www.w3.org/1999/xhtml" id="title-crumbs" style="">
</div>
<h3 xmlns="http://www.w3.org/1999/xhtml" id="sites-page-title-header" style="" align="left">
<span id="sites-page-title" dir="ltr">Special Sessions</span>
</h3>
<div id="sites-canvas-main" class="sites-canvas-main">
<div id="sites-canvas-main-content">
<table xmlns="http://www.w3.org/1999/xhtml" cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr"><span style="line-height:1.5;font-size:1em"><div><div><font size="3"><strong>Special Session Chairs</strong></font></div><div><span style="line-height:1.5;font-size:1em">Dr. Petia Radeva, </span><span style="line-height:1.5;font-size:1em"><a href="mailto:petia@cvc.uab.es">petia@cvc.uab.es</a></span></div><div><span style="line-height:1.5;font-size:1em">Dr. Ji-Zheng Xu, <a href="mailto:jzxu@microsoft.com">jzxu@microsoft.com</a></span></div><table border="0" cellpadding="0" cellspacing="0" style="border:currentColor;border-collapse:collapse"></table></div><div> </div><div> </div><div><strong><font size="3">Special Session 1</font></strong></div><div><strong>Title:</strong> Automatic Indexing of Large-Scale Internet Multimedia</div><div><strong>Organizers:</strong></div><div>　　Qianni Zhang, Queen Mary University of London, <a href="mailto:qianni.zhang@eecs.qmul.ac.uk">qianni.zhang@eecs.qmul.ac.uk</a></div><div>　　Ebroul Izquierdo, Queen Mary University of London, <a href="mailto:ebroul.izquierdo@eecs.qmul.ac.uk">ebroul.izquierdo@eecs.qmul.ac.uk</a> </div><div><strong>Description:</strong></div><div>With the advances in computer technologies and the evolution of World Wide Web, there has been an explosion in the amount and complexity of digital media that is being generated, stored, transmitted and accessed through the Internet. Much of this information is multimedia in nature, including digital images, video, audio, graphics and textual data. Large-scale online image and video repositories enable users to creatively share thoughts among a much wider audience.  As a consequence, every online user has been transformed into the role of a broadcaster. In efforts to exploit this content, there is an increasing interest in associating these media items with free-text annotations describing different facets of the content. The simplest method to generate such annotations is manual tagging. However, this has significant disadvantages which have been studied over the years. The three main problems associated with it include (i) manual labour; (ii) differences in the interpretation of the media items; and (iii) inconsistency of the keyword assignments among tags. Due to these disadvantages, recently there has been large amount of research focusing on automatic indexing of multimedia content on the Internet. In other words, there is currently great interest in the development of techniques that are able to take advantage of the characteristics of Internet multimedia that sets it apart from multimedia in more conventional environments in order to generate effective and useful annotations. </div><div>This special session aims to provide a forum for the presentation of state-of-the-art research results in this emerging field and to address the growing interests in automatic indexing of Internet Multimedia.  The special session will focus on mechanisms capable of exploiting the full range of information available online to automatically assign tags and labels to multimedia data on the Internet. </div><div>The topics of interest for this special session include (but are not limited to):</div><ul><li>Generating Tags</li><li>Exploiting metadata</li><li>Ontology based learning</li><li>Combining modalities</li><li>Confronting noise</li><li>Duplicity and redundancy detection</li><li>Social media mining</li><li>User contributed comments and ratings</li><li>Authority and trust</li><li>Affect detection</li><li>Video analytics for smart cities</li><li>Exploiting external data resources</li><li>Making use of geo-tags and other sensor-based social information</li></ul><div>This special session targets attracting interests from related communities. The aim is to involve participants who are interested in user-aware and multimodal approaches to multimedia involving, e.g., speech recognition, multimedia content analysis, user-contributed information (tags, tweets), viewer affective response, social networks, user modelling and personalised services, geographical mining and social media organisation, etc.</div><div> </div><div> </div><div><strong><font size="3">Special Session 2</font></strong><div><strong>Title:</strong> Cross-media Computing</div><div><strong>Organizers:</strong></div><div>　　Yueting Zhuang, College of Computer Science, Zhejiang University, <a href="mailto:yzhuang@cs.zju.edu.cn">yzhuang@cs.zju.edu.cn</a></div><div>　　Fei Wu, College of Computer Science, Zhejiang University, <a href="mailto:wufei@cs.zju.edu.cn">wufei@cs.zju.edu.cn</a></div><div>　　Zhongfei (Mark) Zhang, Binghamton University, USA, <a href="mailto:zhongfei@cs.binghamton.edu">zhongfei@cs.binghamton.edu</a></div><div><strong>Description:</strong></div><div>Nowadays, there are lots of heterogeneous and homogeneous media data from multiple sources, such as news media websites, microblog, mobile phone, social networking websites, and photo/video sharing websites. These media data are integrated together to reflect our real-world. Consequently, it is impossible to conceive our real-world without exploiting the data available on these different sources of rich multimedia content simultaneously.</div><div>Cross-media is a research area in the general multimedia field which focuses on the utilization of data with different modalities from multiple sources to discover knowledge and understand the real-world. </div><div>Specifically, we emphasize two essential elements in the study of cross-media that help differentiate cross-media from the rest of the research in the general field of multimedia. </div><div>The first is the simultaneous co-existence of data from two or more different data sources. This element indicates the concept of "cross", e.g., cross-modality, cross-source, and cross cyberspace to reality. Cross-modality means heterogeneous features are obtained from data in different modalities; cross-source means the data may be obtained across multiple sources (domains or collections); cross-space means the virtual world (cyberspace) and the real world (reality) complement each other.</div><div>The second is the leverage of different kinds of data across multiple sources for discovering knowledge, for example, discovering the (latent) correlation between data with different modalities across multiple sources, borrowing the strength from data with different modalities to better understand cross-media, generating a summary with data from multiple sources. Cross-media learning is beneficial for many applications in data mining, cross-media retrieval, causal inference, and public security. </div><div>There two essential elements help promote cross-media as a new, emerging, and important research area in today's multimedia research.</div><div>This special session will address the main issues of cross-media computing as follows: Conceptual representation or a high-level modeling for cross-media data; Summarization discovered from cross-media data; Transfer learning across different data sources for cross-media data; Cross-media data search and semantic description; Cross-media data topic modeling; Temporal cross-media data evolutionary analysis and trend prediction; Cyberspace and reality mapping analysis; Cognitive analysis from cyberspace behaviors.</div></div><div> </div><div> </div><div><strong><font size="3">Special Session 3</font></strong></div><div><strong>Title:</strong> Geo-Social Media Mining, Analysis, Recommendation and Retrieval</div><div><strong>Organizers:</strong></div><div><span style="line-height:1.5;font-size:1em;background-color:transparent">      Jailie Shen, Singapore Management University, Singapore, <a href="mailto:jlshen@smu.edu.sg">jlshen@smu.edu.sg</a> </span></div><div>      Xueming Qian, Xi’an Jiaotong University, Xi’an China, <a href="mailto:qianxm@mail.xjtu.edu.cn">qianxm@mail.xjtu.edu.cn</a></div><div>      Peng Cui, Tsinghua University, <a href="mailto:cuip@tsinghua.edu.cn">cuip@tsinghua.edu.cn</a></div><div>      You Yang, Huazhong University of Science and Technology, Wuhan, China, <a href="mailto:youyang.sayu@gmail.com">youyang.sayu@gmail.com</a></div><div><strong>Description:</strong></div><div>This special session is devoted to the publications of high quality papers on technical developments and practical applications around social, geo-Media analysis and retrieval. It will serve as a forum for recent advances in the fields of social and geo-media content analysis, mining, search, and emerging new applications, such as geo-media systems, context-aware advertising, and personalized socio-mobile experience. We invite original and high quality submissions addressing all aspects of this. Relevant topics include, but are not limited to, the following: </div><ul><li>Contextual models for geo- social media analysis </li><li>Novel features for social geo-social media analysis </li><li>Event recognition in social media </li><li>Efficient learning and mining algorithms for geo- social media analysis </li><li>Cloud support for large scale geo-social media analysis and retrieval</li><li>Multimedia indexing and mining on mobile devices </li><li>Social media interaction and visualization on mobile devices </li><li>Location-based social geo-media applications </li><li>Geo- social media applications </li><li>Geo- social media recommendation</li></ul><div> </div><div> </div><div><strong><font size="3">Special Session 4</font></strong><br /><strong>Title:</strong> Human Action and Activity Understanding from Rich Media and Sensors<br /><strong>Organizers:</strong><br />　　Liang Lin, Sun Yat-Sen University, China, <a href="mailto:linliang@ieee.org">linliang@ieee.org</a><br />　　Liangliang Cao, IBM T. J. Watson Research Center, <a href="mailto:liangliang.cao@us.ibm.com">liangliang.cao@us.ibm.com</a><br />　　YingLi Tian, The City University of New York, <a href="mailto:ytian@ccny.cuny.edu">ytian@ccny.cuny.edu</a> <br /><strong>Description:</strong><br />With the popularity of diverse video capturing devices and mobile networks, we are entering an era with rich amount of multimedia data surrounding us. There have been increasing demands of understanding human actions, activities, and events in these rich media data. In the research of Multimedia, there is a particular interest in the last ten years on developing such intelligent systems of action/activity recognition with different application backgrounds, e.g. intelligent surveillance, robotics, video content search. On the other hand, recently developed 3D/depth sensor (e.g., Microsoft Kinect) has opened up new opportunities in many fields.  These advanced sensors can provide more rich information compared with the traditional cameras (e.g., three-dimensional structure information of the scene and the subjects/objects). By taking full advantage of the rich media and sensors, more related systems and approaches with enormous commercial value becoming a new hot topic of inter-disciplines with other research fields such as computer vision, machine learning and network.   <br />We view International Conference on Multimedia Expo 2014 as a great opportunity to organize a special session on "Human Action and Activity Understanding from Rich Media and Sensor". The session will be aimed at presenting the following topics but not limited to: <br />1. Action and activity understanding from rich media<br />    a. New representations for human actions and evens with diverse media<br />    b. New algorithms for inference and learning for recognizing human actions, activities and events <br />    c. New applications centering at human action/even understanding, e.g.<br />        i. Answering queries about various events in a video in natural language <br />        ii. Event prediction <br />        iii. Digital "zoom-in/out" for individual and group activity recognition  <br />2. Action recognition with depth information <br />    a. New representations of depth data for human actions  <br />    b. New algorithms for fusion of RGB data with the depth data for recognizing human actions, activities and events <br />    c. New databases for human action recognition captured by RGBD cameras <br />    d. New applications of depth cameras based human action recognition and activity analysis  <br />3. Industry demos and new applications <br />    a. New systems or live demos in the related topics <br />    b. New exhibitions or live demos<br /></div><div> </div><div> </div><div><strong><font size="3">Special Session 5</font></strong></div><div><strong>Title:</strong> Neuroimaging-guided Multimedia Analysis</div><div><strong>Organizers:</strong></div><div>　　Tianming Liu, The University of Georgia (UGA), USA, <a href="mailto:tliu@cs.uga.edu">tliu@cs.uga.edu</a></div><div>　　Junwei Han, Northwestern Polytechnical University, China, <a href="mailto:junweihan2010@gmail.com">junweihan2010@gmail.com</a>.</div><div>　　Xian-Sheng Hua, Microsoft Research, <a href="mailto:xshua@microsoft.com">xshua@microsoft.com</a></div><div><strong>Description:</strong></div><div>A challenging problem when dealing with multimedia content analysis and modeling is the semantic gap between the high-level perception by human brain and the low-level representation by digital signals/features. Ultimately, multimedia content and semantics are processed and comprehended in the human brain, and better understanding and quantification of how the human brain perceives multimedia will fundamentally advance computational strategies for multimedia representation, classification and retrieval. It is remarkable that the rapid advance of neuroimaging techniques such as functional magnetic resonance imaging (fMRI), electroencephalography (EEG) and magnetoencephalography (MEG), enables us to provide effective and non-invasive approaches to monitoring and probing the human brain in different circumstances such as under natural stimulus of multimedia viewing. Therefore, in recent years, there emerges a new trend that applies brain science principles and brain imaging techniques to meaningfully and informatively guide the multimedia analysis and modeling. This new methodology has been shown to be able to considerably narrow the significant gaps between low-level multimedia features and high-level semantics, and thus significantly speedup the advancement of the multimedia understanding field.  </div><div>This special session will mainly focus on the synergistic combinations of cognitive brain science, brain imaging and multimedia analysis, which can be used to create hybrid multimedia systems. This session is organized upon one major theme, that is, how quantitative functional brain activity modeling can provide new insight and guidance for inferring more intelligent or semantic multimedia representations.   </div><div>This special issue aims at capturing the latest advances by the research community working on neuroimaging-guided computational models for multimedia analysis. We are soliciting original contributions and encouraging the work for   </div><ul><li>Brain encoding and decoding computational models under natural multimedia (image/video/audio) stimulus</li><li>New development of brain computer interface (BCI) systems</li><li>Model of brain functional interaction under natural multimedia stimulus</li><li>Brain-guided multimedia content representation</li><li>Brain-guided multimedia applications, for instances, object recognition, image/video/audio categorization, image/video/audio retrieval and summarization, image/video/audio emotion or effective computing, image/video/audio recommendation, and so on.</li></ul><div> </div><div> </div><div><strong><font size="3">Special Session 6</font></strong><br /><strong>Title:</strong> Visual saliency: emerging models and applications in multimedia processing<br /><strong>Organizers:</strong><br />　　Zhi Liu, Shanghai University, China, <a href="mailto:liuzhi@staff.shu.edu.cn">liuzhi@staff.shu.edu.cn</a><br />　　Olivier Le Meur, University of Rennes 1, France and IRISA, France, <a href="mailto:olivier.le_meur@irisa.fr">olivier.le_meur@irisa.fr</a> <br />　　Xiang Zhang, University of Electronic Science and Technology of China, <a href="mailto:uestchero@uestc.edu.cn">uestchero@uestc.edu.cn</a> <br /><strong>Description:</strong><br />The proposed special session aims at highlighting the emerging visual saliency models and its applications in the field of multimedia processing. It will provide an interdisciplinary forum bridging across eye movements, computational saliency modeling and interesting saliency- based applications. <br />The research on computational saliency models is originally motivated by simulating human visual attention, and a number of saliency models that exploit bottom-up information or/and top-down information have been proposed in the past decades. Except for the well-known biologically plausible center-surround scheme, a variety of well-defined theories and methods such as information theory, control theory, graph theory, frequency domain analysis, machine learning, sparse representation, etc., have been successfully introduced into saliency modeling. Besides, saliency models have been widely used in a number of applications including human fixation prediction, object detection/segmentation/recognition, image/video retargeting, image/video retrieval, image/video editing, content-based compression, visual tracking, active vision, advertising, human-centric analysis, etc. <br />Although recent years witnessed the great development of both saliency models and the related applications, it is still difficult for the state-of-the-art saliency models to achieve a good saliency detection performance on images/videos with complex scenes. Therefore, it is worthy to further explore new information such as high-level attribute cue and specific object semantic cue, tag information of social images, the influence of audio signals in video, to effectively improve the performance of saliency models. Besides, it is also desirable to investigate new methods for interesting saliency-based applications, such as salient object detection using saliency propagation and multiple hypotheses fusion, and co-salient object detection based on low-rank matrix recovery method. <br />We believe that the topic of the proposed session is within the scope of ICME and will be of great interest for ICME participants. Understanding how people perceive the visual environment and modeling the way we look at multimedia content are fundamental for offering the best multimedia technologies.</div><div> </div><div> </div><div> </div><div><strong><font size="3">Submission Guidelines</font></strong></div><div><br />Submissions should be submitted through the manuscript submission system of ICME2014(<a href="index.html">http://www.icme2014.org/</a>). Papers should be formatted according to the guidelines for authors. Each paper will undergoing the same peer review of regular papers.</div><div> </div></span>
<div> </div>
<div>  </div>
<div>
<div style="line-height:1.5;font-size:1em"><span style="line-height:1.5;font-size:1em">
</span></div>
<div><b><font size="3">Important Dates for Special Sessions</font></b></div>
</div>
<div>
<table border="0" cellpadding="0" cellspacing="0" style="border:currentColor;line-height:1.5;font-size:1em;border-collapse:collapse">
<tbody>
<tr>
<td style="padding:0cm 0pt;width:10cm" valign="top" width="300">
<p>
Special session paper abstract submission  <br />
Special session paper submission<br />
Notification of acceptance of papers <br />
Camera-ready papers</p>
</td>
<td style="padding:0cm 0pt;width:120pt" valign="top" width="120">
<p align="left" style="text-align:left"><span style="text-align:left">
</span><span style="text-align:left">
</span><span style="text-align:left">December 3, 2013<br />
</span><span style="text-align:left">December 9, 2013<br />
</span><span style="text-align:left">March 16, 2014<br />
</span><span style="text-align:left">April 16, 2014</span><font size="3"><i><span lang="EN-US" style="font-family:Arial,sans-serif"><br />
</span></i></font></p>
</td>
</tr>
</tbody>
</table>
</div>
<div>
</div><div> </div><div> </div><div> </div></div></td></tr></tbody></table>
</div> 
</div> 
<div id="sites-canvas-bottom-panel">
<div xmlns="http://www.w3.org/1999/xhtml" id="COMP_page-subpages"> </div>
<div id="sites-attachments-container">
</div>
</div>
</div> 
</td> 
</tr>
</table> 
</div> 
</div> 
<div id="sites-chrome-footer-wrapper">
<div id="sites-chrome-footer-wrapper-inside">
<div id="sites-chrome-footer">
<div xmlns="http://www.w3.org/1999/xhtml" class="sites-subfooter"><div class="sites-subfooter-content"><div dir="ltr">Copyright 2012-2014 <a href="http://www.ieee-icme.org/index.php">IEEE ICME</a> | Contact: icme14@gmail.com</div></div></div>
</div>
</div>
</div>
</div> 
</div> 
<div id="sites-chrome-adminfooter-container">
<div xmlns="http://www.w3.org/1999/xhtml" class="sites-adminfooter"><p><a class="sites-system-link" href="system/app/pages/reportAbuse.html" target="_blank">Report Abuse</a>|<span class="sites-system-link">Powered By</span> <b class="powered-by"><a href="http://sites.google.com/">Google Sites</a></b></p></div>
</div>
</div> 
</div> 
<div id="sites-chrome-onebar-footer">
</div>

<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript">
    window.jstiming.load.tick('sjl');
  </script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../www.gstatic.com/sites/p/158cce/system/js/jot_min_view__en.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript">
    window.jstiming.load.tick('jl');
  </script>
<script xmlns="http://www.w3.org/1999/xhtml">
    
        sites.core.Analytics.createTracker();
        sites.core.Analytics.trackPageview();
      
  </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript">
      gsites.HoverPopupMenu.createSiteDropdownMenus('sites-header-nav-dropdown', false);
    </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
            JOT_setupNav("2bd", "Conference", false);
            JOT_addListener('titleChange', 'JOT_NAVIGATION_titleChange', 'COMP_2bd');
          </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
            JOT_setupNav("39694875036366284", "Program", false);
            JOT_addListener('titleChange', 'JOT_NAVIGATION_titleChange', 'COMP_39694875036366284');
          </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
            JOT_setupNav("14756362972374581", "Industry Program", false);
            JOT_addListener('titleChange', 'JOT_NAVIGATION_titleChange', 'COMP_14756362972374581');
          </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
            JOT_setupNav("5307298570405692", "Author", false);
            JOT_addListener('titleChange', 'JOT_NAVIGATION_titleChange', 'COMP_5307298570405692');
          </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
            JOT_setupNav("7836366717237979", "Attendee", false);
            JOT_addListener('titleChange', 'JOT_NAVIGATION_titleChange', 'COMP_7836366717237979');
          </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
            JOT_setupNav("5951473142486066", "Other", false);
            JOT_addListener('titleChange', 'JOT_NAVIGATION_titleChange', 'COMP_5951473142486066');
          </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript">
  setTimeout(function() {
    var fingerprint = gsites.date.TimeZone.getFingerprint([]);
    gsites.Xhr.send('_/tz.html', null, null, 'GET', null, null, { afjstz: fingerprint });
  }, 500);
</script>
<script xmlns="http://www.w3.org/1999/xhtml">
                    window.onload = function() {
                      if (false) {
                        JOT_setMobilePreview();
                      }
                      var loadTimer = window.jstiming.load;
                      loadTimer.tick("ol");
                      loadTimer["name"] = "load," + webspace.page.type + ",user_page";
                      window.jstiming.report(loadTimer, {}, 'http://csi.gstatic.com/csi');
                    }
                  </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript">
    var maestroRunner = new gsites.pages.view.SitesMaestroRunner(
        webspace, "en");
    maestroRunner.initListeners();
    maestroRunner.installEditRender();
  </script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript" defer="true">
  //<![CDATA[
    // Decorate any fastUI buttons on the page with a class of 'goog-button'.
    if (webspace.user.hasWriteAccess) {
      JOT_decorateButtons();
    }

    // Fires delayed events.
    (function() {
      JOT_fullyLoaded = true;
      var delayedEvents = JOT_delayedEvents;
      for (var x = 0; x < delayedEvents.length; x++) {
        var event = delayedEvents[x];
        JOT_postEvent(event.eventName, event.eventSrc, event.payload);
      }
      JOT_delayedEvents = null;
      JOT_postEvent('pageLoaded');
    })();
 //]]>
</script>
<script xmlns="http://www.w3.org/1999/xhtml" type="text/javascript">
    JOT_postEvent('decorateGvizCharts');
  </script>
<script type="text/javascript">
          if (webspace.features.siteNotice) {
            JOT_setupNoticeManager();
          }
        </script>
<script type="text/javascript">
              JOT_postEvent('renderPlus', null, 'sites-chrome-main');
            </script>
<div id="server-timer-div" style="display:none"> </div>
<script type="text/javascript">
          window.jstiming.load.tick('render');
          JOT_postEvent('usercontentrendered', this);
        </script>
</body>

<!-- Mirrored from www.icme2014.org/special-sessions by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 03 Jul 2014 11:52:07 GMT -->
</html>
