<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" itemscope="" itemtype="http://schema.org/WebPage"><head><!-- Mirrored from www.icme2014.org/call-for-workshop-proposals by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 03 Jul 2014 11:51:40 GMT --><!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->    
<meta http-equiv="X-UA-Compatible" content="chrome=1" />

<link rel="canonical" href="specialsessions.html" />
<meta name="title" content="Special Sessions - ICME 2016" />
<meta itemprop="name" content="Special Sessions - ICME 2016" />
<meta property="og:title" content="Special Sessions - ICME 2016" />
<meta name="description" content="The 2016 IEEE International Conference on Multimedia &amp; Expo" />
<meta itemprop="description" content="The 2016 IEEE International Conference on Multimedia &amp; Expo" />
<meta id="meta-tag-description" property="og:description" content="The 2016 IEEE International Conference on Multimedia &amp; Expo" />

<style type="text/css">
    .auto-style1 {
        text-align: justify;
    }


    .auto-style2 {
        border-collapse: collapse;
        font-size: 11.0pt;
        font-family: Calibri, sans-serif;
        border: 1.0pt solid windowtext;
    }


    </style>
<link rel="stylesheet" type="text/css" href="../www.gstatic.com/sites/p/158cce/system/app/themes/solitudenavy/standard-css-solitudenavy-ltr-ltr.css" />
<link rel="stylesheet" type="text/css" href="_/rsrc/1403685870000/system/app/css/overlay6de5.css?cb=solitudenavy1000px200goog-ws-leftcontent30middlecenter" />
<link rel="stylesheet" type="text/css" href="_/rsrc/1403685870000/system/app/css/camelot/allthemes-view.css" /><!--[if IE]>
          <link rel="stylesheet" type="text/css" href="/system/app/css/camelot/allthemes%2die.css" />
        <![endif]-->



<title>Registration - ICME 2016</title><link rel="stylesheet" type="text/css" href="css/body.css" /></head>




<body xmlns="http://www.google.com/ns/jotspot" id="body" class="newStyle1">
    <div id="sites-canvas">
<div id="goog-ws-editor-toolbar-container"> </div>
<div xmlns="http://www.w3.org/1999/xhtml" id="title-crumbs" style="">
</div>
<h3 xmlns="http://www.w3.org/1999/xhtml" id="sites-page-title-header" style="" align="center">
    Special Sessions
</h3>
<div id="sites-canvas-main-content">
<table xmlns="http://www.w3.org/1999/xhtml" class="sites-layout-name-one-column sites-layout-hbox" cellspacing="0">
    <tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr"><div> </div>
<br />
<div class="auto-style1" style="line-height: 1.5; font-size: 1em;"><div>
        <!--<center> <a href="files/schedule/july11.png" target="_blank"><img width="900" height="400" src="files/schedule/july11.png" /></a></center>-->

<br /><b><span style="font-size: 13.5pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Free Navigation and Immersive 3D<o:p></o:p></span></b>

<br /><u><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Tuesday, July 12,
14:30-15:50<o:p></o:p></span></u>

<br /><i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Masayuki Tanimoto, Nagoya Industrial Science
Research Institute, Japan</span></i><br /><i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">
Gauthier Lafruit, Université Libre de Bruxelles, Belgium</span></i><br /><i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">
Dr. Joël Jung, IRT B-COM, France </span></i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Eager to provide the
user a rich multimedia experience, companies constantly innovate in novel
display modalities, introducing advanced digital image post-processing into the
rendering pipeline. So far, however, most viewing remains limited to a single
or stereoscopic visualization with little immersive experience, excluding
motion parallax and Free Navigation features as offered in the real world.<o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Today, computational
light field cameras already offer narrow, user-perspective parallax rendering
capabilities. Tomorrow, Free-viewpoint Television (FTV) will give the user
complete freedom in choosing any viewpoint within a multi-camera-enclosed
volume, reaching the ultimate, immersive experience. <o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">A key challenge
resides in developing realistic technology and image processing methods that
support the synthesis of novel views at rendering time with correct occlusion
handling, inpainting and data recovery mechanisms from sparse camera
arrangements. Standardization activities such as MPEG-FTV and JPEG-PLENO are
currently exploring these challenges. <o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">This special session
explores new advances in Free Navigation and Immersive 3D technologies.
Ground-breaking scientific and technological papers are solicited, bringing new
insights in this era. The paper subjects range from new capture and display
technologies to novel mathematical insights in light field data sampling and
representation, mixing diverse modalities (point clouds, image-based rendering,
plenoptic processing), as well as improvements in compression formats.</span><br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"></span><br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"></span><table style="border-style: groove; border-width: thin;" align="center" border="1"><tbody><tr><td><big>698</big></td><td width="336"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Efficient
MRF-based disocclusion inpainting in multiview video</span></big></big></big></big></td><td width="397"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Beerend
Ceulemans, Vrije Universiteit Brussel; Shao-Ping Lu, Vrije Universiteit
Brussel; Gauthier<span style="">&nbsp; </span>Lafruit, Université
Libre de Bruxelles, Belgium; Peter Schelkens, Vrije Universiteit Brussel, Belgium;
Adrian Munteanu, Vrije Universiteit Brussel</span></big></big></big></big></td></tr><tr><td><big>669</big></td><td width="336"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">New
Results in Free-Viewpoint Television Systems for Horizontal Virtual Navigation</span></big></big></big></big></td><td width="397"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Marek
Domanski, Poznan University of Technology; Maciej Bartkowiak, Poznan University
of Technology; Adrian Dziembowski, Poznan University of Technology; Tomasz
Grajek, Poznan University of Technology; Adam Grzelka, Poznan University of
Technology; Adam Luczak, Poznan University of Technology; Dawid Mieloch, Poznan
University of Technology; jaroslaw Samelak, Poznan University of Technology;
Olgierd Stankiewicz, Poznan University of Technology; Jakub Stankowski, Poznan
University of Technology; Krzysztof Wegner, Poznan University of Technology</span></big></big></big></big></td></tr><tr><td><big>367</big></td><td width="336"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">EFFICIENT
PLENOPTIC IMAGING REPRESENTATION: WHY DO WE NEED IT ?</span></big></big></big></big></td><td width="397"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Fernando
Pereira, IST-IT; Eduardo Silva, UFRJ</span></big></big></big></big></td></tr><tr><td><big>492</big></td><td width="336"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Content-adaptive
focus configuration for near-eye multi-focal displays</span></big></big></big></big></td><td width="397"><big><big><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Wanmin
Wu, Ricoh, USA; Patrick<span style="">&nbsp; </span>Llull, Duke
University; Ivana Tosic, Ricoh Innovations Corp; Noah Bedard, Ricoh Innovations
Corp; Kathrin Berkner, Ricoh Innovations Corp; Nikhil Balram, Ricoh Innovations
Corp</span></big></big></big></big></td></tr></tbody></table><br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>
</div><br /><div>

<b><span style="font-size: 13.5pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Multimedia Big Data and Cloud Computing </span></b><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><u><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Tuesday, July 12,
17:00-18:00<o:p></o:p></span></u>

<br /><i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Zheng-Jun Zha, University of Science and
Technology of China, China<br />
Gwendal Simon, Telecom Bretagne, Institute Mine Telecom, France<br />
Shervin Shirmohammadi, University of Ottawa, Canada<br />
Xiaokang Yang, Shanghai Jiao Tong University, China </span></i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">“Big Data” has become
a ubiquitous term in recent years and multimedia is becoming the “biggest Big
Data.” Multimedia big data comes from sources as varied as security cameras,
medical imaging, and individuals sharing media on Internet. It is the most important
and valuable source for insights and information. Multimedia big data is
spurring on huge amounts of research and development of related technologies
and applications. <o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">To deal with this
tremendous amount of data to process, another term has become ubiquitous:
"cloud computing." This mainstream idiom encompasses a set of
hardware and software technologies, which have all in common that they deal
with multimedia content. Still in their infancy, they also have to evolve to
meet the demand for more computing and, eventually more QoE for the end-users,
with respect to global energy consumption.<o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">A key challenge
resides in developing realistic technology and image processing methods that
support the synthesis of novel views at rendering time with correct occlusion
handling, inpainting and data recovery mechanisms from sparse camera
arrangements. Standardization activities such as MPEG-FTV and JPEG-PLENO are
currently exploring these challenges. <br /><br />This
special session will provide a venue for the participants to discuss
key research issues on multimedia big data and cloud computing. It will
collect and seek the recent important research works in this area,
summarize the available resources, and exploit potential challenges and
possible advanced solutions in terms of theory and practice.<br /></span><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span><br />

<span style="font-size: 11pt; line-height: 107%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><br /></span><table style="border-style: groove; border-width: thin;" align="center" border="1"><tbody><tr><td><big>446</big></td>
                    <td width="336"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">A
Comparative Evaluation: Different Methods for Simplifying the Deep
Compositional Features</span></big></big></td>
                    <td width="397"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Shuang
Qiu, Beijing Jiaotong University; Shikui Wei, Beijing Jiaotong University; Yao
Zhao, Beijing Jiaotong University</span></big></big></td>
                </tr>
                <tr>
                    <td><big>138</big></td>
                    <td width="336"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Person
Re-identification via Rich Color-gradient Feature</span></big></big></td>
                    <td width="397"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">LINGXIANG
WU, CASIA NLPR; Jinqiao Wang, National lab of Automation; guibo Zhu, ; Min Xu,
Univeristy of Technology Sydney; Hanqing Lu </span></big></big></td>
                </tr>
                <tr>
                    <td><big>524</big></td>
                    <td width="336"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">A
Graph-based Geospatial Multimodal Interpolation Framework</span></big></big></td>
                    <td width="397"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Mengfan
Tang, University of California, Irvine; Pranav Agrawal, University of
California, Irvine; Feiping Nie, Northwestern Polytechnical University; Siripen
Pongpaichet, ; Ramesh Jain, University of California, Irvine</span></big></big></td>
                </tr>
                
                
            </tbody></table><br /><br />

<b><span style="font-size: 13.5pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Quality Assessment</span></b><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><u><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Wednesday, July 13,
13:20-15:00<o:p></o:p></span></u>

<br /><i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Yuming Fang, Jiangxi University of Finance and
Economics, Nanchang, China<br />
Hantao Liu, Cardiff University, UK <br />
Zhenzhong Chen, Wuhan University, China <br />
Weisi Lin, Nanyang Technological University, Singapore </span></i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Delivering an
optimized quality of experience (QoE) to the end user is a fundamental
challenge for various multimedia communication systems. As we know, most of the
visual information is finally consumed by the Human Visual System (HVS). Thus,
visual processing always involves various factors related to human perception,
human emotion and behaviour, human experiences as well as the characteristics
of visual processing systems/techniques. To provide a promising QoE model for
various multimedia processing applications, we have to investigate these
perceptual and system factors. <o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">This special session covers
the latest technology developments in the areas of perceptual visual
processing, as well as the potential applications and systems in the related
area. It mainly focuses on the new developments and applications of visual
attention and visual quality assessment. Specifically, for the visual quality
assessment, besides the general-purpose visual quality assessment (VQA), we
will also focus on the emerging quality assessment for various specific
applications, such as VQA for screen content images (SCI), graphics, high
dynamic ranging (HDR) images, high definition television video (HDTV), 3D
images/video, etc.<o:p></o:p></span><br />
<br /><table style="border-style: groove; border-width: thin;" align="center" border="1"><tbody><tr><td width="52">102</td>
                    <td width="338"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Quality-of-Experience
Prediction for Streaming Video</span></big></big></td>
                    <td width="390"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Zhengfang
Duanmu, University of Waterloo; Abdul Rehman, University of Waterloo; Kai Zeng,
University of Waterloo; Zhou Wang, Dept. of Electrical &amp; Computer
Engineering, University of Waterlo</span></big></big></td>
                </tr>
                <tr>
                    <td width="52">455</td>
                    <td width="338"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Blind
Quality Assessment of Compressed Images via Pseudo Structural Similarity</span></big></big></td>
                    <td width="390"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Xiongkuo
Min, Shanghai Jiao Tong University; Guangtao Zhai, ; Ke Gu, ; Yuming Fang,
Nanyang Technological University</span></big></big></td>
                </tr>
                <tr>
                    <td width="52">687</td>
                    <td width="338"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">No-reference
image quality assessment based on high order derivatives</span></big></big></td>
                    <td width="390"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Qiaohong
Li, NTU; Weisi Lin, Nanyang Technological University; Yuming Fang, Nanyang
Technological University</span></big></big></td>
                </tr>
                <tr>
                    <td width="52">564</td>
                    <td width="338"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">FULL-REFERENCE
PERCEPTUAL QUALITY ASSESSMENT FOR STEREOSCOPIC IMAGES BASED ON PRIMARY VISUAL
PROCESSING MECHANISM</span></big></big></td>
                    <td width="390"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Yu
Cao, ; Wenhao Hong, Zhejiang University; Lu<span style="">&nbsp;&nbsp;
</span>Yu, Zhejiang University, China</span></big></big></td>
                </tr>
                <tr>
                    <td width="52">164</td>
                    <td width="338"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">LEARNING-BASED
QUALITY ASSESSMENT OF RETARGETED STEREOSCOPIC IMAGES</span></big></big></td>
                    <td width="390"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Yi
Liu, Tsinghua University; Lifeng<span style="">&nbsp;&nbsp; </span>Sun,
Tsinghua University, China; shiqiang yang, Tsinghua University</span></big></big></td>
                </tr>
            </tbody></table>

<b><span style="font-size: 13.5pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><br /><br />Deep Learning for Multimedia Computing</span></b><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><u><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Thursday, July 14,
16:00-17:20<o:p></o:p></span></u>

<br /><i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Benoit Huet, Eurecom, France<br />
Jiebo Luo, University of Rochester, USA<br />
Guo-Jun Qi, University of Central Florida, USA </span></i><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">Conventional
multimedia computing is often built on top of handcrafted features, which are
often much restrictive in capturing complex multimedia content such as images,
audios, text and user-generated data with domain-specific knowledge. Recent
progress on deep learning opens an exciting new era, placing multimedia
computing on a more rigorous foundation with automatically learned
representations to model the multimodal data and the cross-media interactions.
Existing studies have revealed promising results that have greatly advanced the
state-of-the-art performance in a series of multimedia research areas, from the
multimedia content analysis, to modeling the interactions between multimodal
data, to multimedia content recommendation systems, to name a few here. <o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US">This ICME 2016 Special
Session aims at providing a forum to present recent advancements in deep
learning research that directly concerns the multimedia community.
Specifically, deep learning has successfully designed algorithms that can build
deep nonlinear representations to mimic how the brain perceives and understands
multimodal information, ranging from low-level signals like images and audios,
to high-level semantic data like natural language. For multimedia research, it
is especially important to develop deep networks to capture the dependencies
between different genres of data, building joint deep representation for
diverse modalities.<o:p></o:p></span>

<br /><span style="font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: rgb(80, 80, 80);" lang="EN-US"><o:p>&nbsp;</o:p></span>
<table style="border-style: groove; border-width: thin;" align="center" border="1"><tbody><tr><td width="51">316</td>
                <td width="248"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Deep
Conditional Neural Network for Image Segmentation</span></big></big></td>
                <td width="480"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Qiurui
Wang, Tsinghua University; Chun Yuan, Tsinghua university; Yan Liu, Hong Kong
Polytechnic University</span></big></big></td>
            </tr>
            <tr>
                <td width="51">314</td>
                <td width="248"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">On-premise
Signs Detection and Recognition Using Fully Convolutional Networks</span></big></big></td>
                <td width="480"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Yong-Xiang
Wang, National Cheng Kung University; Chih-Hsin Hsueh, ; Hung-Yi Lo, ; Min-Chun
Hu, National Cheng Kung University, Taiwan</span></big></big></td>
            </tr>
            <tr>
                <td width="51">33</td>
                <td width="248"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">DEEP
ATTRIBUTE-EMBEDDING GRAPH RANKING FOR CROWD VIDEO RETRIEVAL</span></big></big></td>
                <td width="480"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Yanhao
Zhang, Harbin Institute of Technology; Lei Qin, Institute of Computing, Chinese
Academy of Sciences ; Sicheng Zhao, Harbin Institute of Technology; Rongrong
Ji, Xiamen University; Xiusheng Lu, Harbin Institute of Technology; Hongxun
Yao, Harbin Institute of Technology; Qingming Huang, University of Chinese
Academy of Science</span></big></big></td>
            </tr>
            <tr>
                <td width="51">577</td>
                <td width="248"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">MULTIMEDIA
EVENT DETECTION VIA DEEP SPATIAL-TEMPORAL NEURAL NETWORKS</span></big></big></td>
                <td width="480"><big><big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;" lang="EN-US">Jingyi
Hou, Beijing Institute of Technolog; Xinxiao Wu, Beijing Institute of
Technology; feiwu Yu, ; Yunde Jia, Beijing Institute of Technology</span></big></big></td>
            </tr>
        </tbody></table></div><br /></div>
    </div></td></tr></tbody></table>
</div>

</div> 

</body></html>